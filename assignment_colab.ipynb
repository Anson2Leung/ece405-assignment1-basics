{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Anson2Leung/ece405-assignment1-basics/blob/main/assignment_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eGV11RtlneMZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zo5L1A8AoaRO"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "joDoJG0kpIIl",
    "outputId": "15167997-899d-4b90-bdb4-d4468b91acbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ece405-assignment1-basics' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Anson2Leung/ece405-assignment1-basics.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5DUpEUxfocVo",
    "outputId": "6608f944-ae67-49b4-e943-e418d71c4d07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading uv 0.9.28 x86_64-unknown-linux-gnu\n",
      "no checksums to verify\n",
      "installing to /usr/local/bin\n",
      "  uv\n",
      "  uvx\n",
      "everything's installed!\n"
     ]
    }
   ],
   "source": [
    "# Install uv\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Add uv to the system path so you can use it directly\n",
    "import os\n",
    "os.environ['PATH'] = f\"{os.path.expanduser('~/.cargo/bin')}:{os.environ['PATH']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9katG4dGonsN",
    "outputId": "5cdc8286-5fb6-42ce-fec1-cde4c0bbc8a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/ece405-assignment1-basics\n",
      "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m3 packages\u001b[0m \u001b[2min 108ms\u001b[0m\u001b[0m\n",
      "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
      "Requirement already satisfied: jaxtyping in /usr/local/lib/python3.12/dist-packages (0.3.7)\n",
      "Requirement already satisfied: typeguard in /usr/local/lib/python3.12/dist-packages (4.4.4)\n",
      "Requirement already satisfied: langsmith in /usr/local/lib/python3.12/dist-packages (0.6.6)\n",
      "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from jaxtyping) (0.1.7)\n",
      "Requirement already satisfied: typing_extensions>=4.14.0 in /usr/local/lib/python3.12/dist-packages (from typeguard) (4.15.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith) (3.11.5)\n",
      "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langsmith) (25.0)\n",
      "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langsmith) (2.12.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (2.32.4)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (0.14.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (0.25.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langsmith) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langsmith) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langsmith) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith) (2.5.0)\n"
     ]
    }
   ],
   "source": [
    "%cd ece405-assignment1-basics\n",
    "\n",
    "!uv pip install -r requirements.txt --system\n",
    "!pip install \"numpy<2.0\" jaxtyping typeguard langsmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAa-oCqUovFW"
   },
   "source": [
    "# Test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eISqsOkjowXA",
    "outputId": "03d8c6da-ec52-40e7-ae28-6d1e7f6a93e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0\n",
      "rootdir: /content/ece405-assignment1-basics\n",
      "configfile: pytest.ini\n",
      "plugins: jaxtyping-0.3.7, anyio-4.12.1, typeguard-4.4.4, langsmith-0.6.6\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 3 items                                                              \u001b[0m\n",
      "\n",
      "tests/test_train_bpe.py::test_train_bpe_speed \u001b[32mPASSED\u001b[0m\u001b[32m                     [ 33%]\u001b[0m\n",
      "tests/test_train_bpe.py::test_train_bpe \u001b[32mPASSED\u001b[0m\u001b[32m                           [ 66%]\u001b[0m\n",
      "tests/test_train_bpe.py::test_train_bpe_special_tokens \u001b[31mFAILED\u001b[0m\u001b[31m            [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m________________________ test_train_bpe_special_tokens _________________________\u001b[0m\n",
      "\n",
      "snapshot = <tests.conftest.Snapshot object at 0x7d0bcca5ca70>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_train_bpe_special_tokens\u001b[39;49;00m(snapshot):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Ensure that the special tokens are added to the vocabulary and not\u001b[39;49;00m\n",
      "    \u001b[33m    merged with other tokens.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        input_path = FIXTURES_PATH / \u001b[33m\"\u001b[39;49;00m\u001b[33mtinystories_sample_5M.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        vocab, merges = run_train_bpe(\u001b[90m\u001b[39;49;00m\n",
      "            input_path=input_path,\u001b[90m\u001b[39;49;00m\n",
      "            vocab_size=\u001b[94m1000\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            special_tokens=[\u001b[33m\"\u001b[39;49;00m\u001b[33m<|endoftext|>\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Check that the special token is not in the vocab\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        vocabs_without_specials = [word \u001b[94mfor\u001b[39;49;00m word \u001b[95min\u001b[39;49;00m vocab.values() \u001b[94mif\u001b[39;49;00m word != \u001b[33mb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m<|endoftext|>\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m word_bytes \u001b[95min\u001b[39;49;00m vocabs_without_specials:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94massert\u001b[39;49;00m \u001b[33mb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m<|\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[95min\u001b[39;49;00m word_bytes\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       snapshot.assert_match(\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mvocab_keys\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[96mset\u001b[39;49;00m(vocab.keys()),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mvocab_values\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[96mset\u001b[39;49;00m(vocab.values()),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mmerges\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: merges,\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_train_bpe.py\u001b[0m:82: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <tests.conftest.Snapshot object at 0x7d0bcca5ca70>\n",
      "actual = {'merges': [(b'h', b'e'), (b' ', b't'), (b' ', b'a'), (b' ', b's'), (b' ', b'w'), (b'n', b'd'), ...], 'vocab_keys': {0, 1, 2, 3, 4, 5, ...}, 'vocab_values': {b'\\x00', b'\\x01', b'\\x02', b'\\x03', b'\\x04', b'\\x05', ...}}\n",
      "test_name = 'test_train_bpe_special_tokens', force_update = False\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92massert_match\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        actual: _A | \u001b[96mdict\u001b[39;49;00m[\u001b[96mstr\u001b[39;49;00m, _A],\u001b[90m\u001b[39;49;00m\n",
      "        test_name: \u001b[96mstr\u001b[39;49;00m | \u001b[96mtype\u001b[39;49;00m[DEFAULT] = DEFAULT,\u001b[90m\u001b[39;49;00m\n",
      "        force_update: \u001b[96mbool\u001b[39;49;00m | \u001b[96mtype\u001b[39;49;00m[DEFAULT] = DEFAULT,\u001b[90m\u001b[39;49;00m\n",
      "    ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Assert that the actual data matches the snapshot.\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        actual: Single object or dictionary of named objects\u001b[39;49;00m\n",
      "    \u001b[33m        test_name: The name of the test (used for the snapshot file)\u001b[39;49;00m\n",
      "    \u001b[33m        force_update: If True, update the snapshot instead of comparing\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m force_update \u001b[95mis\u001b[39;49;00m DEFAULT:\u001b[90m\u001b[39;49;00m\n",
      "            force_update = \u001b[96mself\u001b[39;49;00m.default_force_update\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m test_name \u001b[95mis\u001b[39;49;00m DEFAULT:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.default_test_name \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mTest name must be provided or set as default\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            test_name = \u001b[96mself\u001b[39;49;00m.default_test_name\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        snapshot_path = \u001b[96mself\u001b[39;49;00m._get_snapshot_path(test_name)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Load the snapshot\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(snapshot_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            expected_data = pickle.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(actual, \u001b[96mdict\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mfor\u001b[39;49;00m key \u001b[95min\u001b[39;49;00m actual:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m key \u001b[95mnot\u001b[39;49;00m \u001b[95min\u001b[39;49;00m expected_data:\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mraise\u001b[39;49;00m \u001b[96mAssertionError\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mKey \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mkey\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m not found in snapshot for \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtest_name\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      ">               \u001b[94massert\u001b[39;49;00m actual[key] == expected_data[key], (\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mData for key \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mkey\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m does not match snapshot for \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtest_name\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE               AssertionError: Data for key 'vocab_values' does not match snapshot for test_train_bpe_special_tokens\u001b[0m\n",
      "\u001b[1m\u001b[31mE               assert {b'\\x00', b'\\... b'\\x05', ...} == {b'\\x00', b'\\... b'\\x05', ...}\u001b[0m\n",
      "\u001b[1m\u001b[31mE                 \u001b[0m\n",
      "\u001b[1m\u001b[31mE                 Extra items in the left set:\u001b[0m\n",
      "\u001b[1m\u001b[31mE                 \u001b[0m\u001b[33mb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m rem\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\u001b[0m\n",
      "\u001b[1m\u001b[31mE                 \u001b[0m\u001b[33mb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mtle\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\u001b[0m\n",
      "\u001b[1m\u001b[31mE                 \u001b[0m\u001b[33mb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mimb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\u001b[0m\n",
      "\u001b[1m\u001b[31mE                 \u001b[0m\u001b[33mb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m open\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\u001b[0m\n",
      "\u001b[1m\u001b[31mE                 \u001b[0m\u001b[33mb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m while\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m...\u001b[0m\n",
      "\u001b[1m\u001b[31mE                 \u001b[0m\n",
      "\u001b[1m\u001b[31mE                 ...Full output truncated (1105 lines hidden), use '-vv' to show\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/conftest.py\u001b[0m:146: AssertionError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "Dataset:  /content/ece405-assignment1-basics/tests/fixtures/tinystories_sample_5M.txt\n",
      "Number of processes requested:  2\n",
      "Special tokens as bytes:  [b'<|endoftext|>']\n",
      "Total number of tasks to complete:  1\n",
      "Pre-tokenization complete. Unique pre-tokens: 6412\n",
      "Number of merges to perform:  743\n",
      "Pre-tokenize time complete in 1.44s\n",
      "Initialization time complete in 0.07s\n",
      "BPE Merges complete in 0.55s\n",
      "Total training time: 2.06s\n",
      "Total memory used: 3.75 MB\n",
      "----------------------------- Captured stderr call -----------------------------\n",
      "Merging Vocab: 100%|██████████| 743/743 [00:00<00:00, 1342.83it/s]\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/test_train_bpe.py::\u001b[1mtest_train_bpe_special_tokens\u001b[0m - AssertionError: Data for key 'vocab_values' does not match snapshot for tes...\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m2 passed\u001b[0m\u001b[31m in 3.37s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest tests/test_train_bpe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4D4JapoarrRF"
   },
   "source": [
    "# Problem (train_bpe_tinystories): BPE Training on TinyStories (2 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-BFttVqruMA",
    "outputId": "88fe0b93-25e0-4650-cacc-41e5f3708700"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ansonl32/ECE405\n",
      "/home/ansonl32/ECE405/data\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%mkdir -p data\n",
    "%cd data\n",
    "\n",
    "# !wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt\n",
    "# !wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-valid.txt\n",
    "\n",
    "# !wget https://huggingface.co/datasets/stanford-cs336/owt-sample/resolve/main/owt_train.txt.gz\n",
    "!gunzip owt_train.txt.gz\n",
    "!wget https://huggingface.co/datasets/stanford-cs336/owt-sample/resolve/main/owt_valid.txt.gz\n",
    "!gunzip owt_valid.txt.gz\n",
    "\n",
    "%cd ..\n",
    "%cd ece405-assignment1-basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0eq31Sgitcz2",
    "outputId": "2c44a0f7-82ff-4bb4-b530-82f0ef46bd52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/ece405-assignment1-basics\n",
      "Dataset:  /content/data/TinyStoriesV2-GPT4-train.txt\n",
      "Number of processes requested:  2\n",
      "Special tokens as bytes:  [b'<|endoftext|>']\n",
      "Total number of tasks to complete:  1\n",
      "Pre-tokenization complete. Unique pre-tokens: 47063\n",
      "Number of merges to perform:  9743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging Vocab: 100%|██████████| 9743/9743 [00:36<00:00, 267.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-tokenize time complete in 472.79s\n",
      "Initialization time complete in 0.95s\n",
      "BPE Merges complete in 36.43s\n",
      "Total training time: 510.17s\n",
      "Total memory used: 81.02 MB\n",
      "Vocab saved to TinyStories_vocab.json\n",
      "Merges saved to TinyStories_merges.txt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "%cd /content/ece405-assignment1-basics\n",
    "sys.path.append('/content/ece405-assignment1-basics/assignment_files')\n",
    "from bpe_tokenizer import train_bpe, save_vocab, save_merges\n",
    "\n",
    "INPUT_PATH = \"/content/data/TinyStoriesV2-GPT4-train.txt\"\n",
    "VOCAB_SIZE = 10000\n",
    "SPECIAL_TOKENS = [\"<|endoftext|>\"]\n",
    "\n",
    "vocab, merges = train_bpe(INPUT_PATH, VOCAB_SIZE, SPECIAL_TOKENS)\n",
    "\n",
    "save_vocab(vocab, \"TinyStories_vocab.json\")\n",
    "save_merges(merges, \"TinyStories_merges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "imB5DHm7tyM5",
    "outputId": "7dba04c4-3459-4485-b6fa-7e7b65c481cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Analysis Results ---\n",
      "Longest Token ID: 7107\n",
      "Longest Token Text: ' accomplishment'\n",
      "Character Length: 15\n"
     ]
    }
   ],
   "source": [
    "def get_decoded_string(token_bytes):\n",
    "    return token_bytes.decode('utf-8', errors='ignore')\n",
    "\n",
    "# Find the longest token in the vocabulary\n",
    "longest_token_id = max(vocab, key=lambda k: len(get_decoded_string(vocab[k])))\n",
    "longest_token_bytes = vocab[longest_token_id]\n",
    "longest_text = get_decoded_string(longest_token_bytes)\n",
    "\n",
    "print(f\"--- Analysis Results ---\")\n",
    "print(f\"Longest Token ID: {longest_token_id}\")\n",
    "print(f\"Longest Token Text: '{longest_text}'\")\n",
    "print(f\"Character Length: {len(longest_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFREVD0s6Tx-"
   },
   "source": [
    "# Problem (train_bpe_expts_owt): BPE Training on OpenWebText (2 points\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nB7ngD7-6UML",
    "outputId": "03f6d239-644c-4252-a9e8-168f894327b9"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/ece405-assignment1-basics\n",
      "Dataset:  /content/data/owt_train.txt\n",
      "Number of processes requested:  2\n",
      "Special tokens as bytes:  [b'<|endoftext|>']\n",
      "Total number of tasks to complete:  1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "%cd /content/ece405-assignment1-basics\n",
    "sys.path.append('/content/ece405-assignment1-basics/assignment_files')\n",
    "from bpe_tokenizer import train_bpe, save_vocab, save_merges\n",
    "\n",
    "INPUT_PATH = \"/content/data/owt_train.txt\"\n",
    "VOCAB_SIZE = 32000\n",
    "SPECIAL_TOKENS = [\"<|endoftext|>\"]\n",
    "\n",
    "vocab2, merges2 = train_bpe(INPUT_PATH, VOCAB_SIZE, SPECIAL_TOKENS)\n",
    "\n",
    "save_vocab(vocab2, \"OWT_vocab.json\")\n",
    "save_merges(merges2, \"OWT_merges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwZEOtur69sc"
   },
   "outputs": [],
   "source": [
    "# Analysis\n",
    "def get_decoded_string(token_bytes):\n",
    "    return token_bytes.decode('utf-8', errors='ignore')\n",
    "\n",
    "# Find the longest token in the vocabulary\n",
    "longest_token_id = max(vocab2, key=lambda k: len(get_decoded_string(vocab2[k])))\n",
    "longest_token_bytes = vocab2[longest_token_id]\n",
    "longest_text = get_decoded_string(longest_token_bytes)\n",
    "\n",
    "print(f\"--- Analysis Results ---\")\n",
    "print(f\"Longest Token ID: {longest_token_id}\")\n",
    "print(f\"Longest Token Text: '{longest_text}'\")\n",
    "print(f\"Character Length: {len(longest_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem (tokenizer_experiments): Experiments with tokenizers (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_documents(filepath: str, n_samples: int = 10):\n",
    "    samples = []\n",
    "    separator = \"<|endoftext|>\"\n",
    "    \n",
    "    print(f\"Sampling {n_samples} documents from {filepath}...\")\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        # Read a large enough chunk to get 10 stories (TinyStories are short)\n",
    "        # 100KB is plenty for 10 stories.\n",
    "        content = f.read(100_000) \n",
    "        \n",
    "        # Split by the special token\n",
    "        raw_docs = content.split(separator)\n",
    "        \n",
    "        for doc in raw_docs:\n",
    "            clean_doc = doc.strip() # Removes the '\\n' at Line 1 and Line 6\n",
    "            if clean_doc:\n",
    "                samples.append(clean_doc)\n",
    "            \n",
    "            if len(samples) >= n_samples:\n",
    "                break\n",
    "                \n",
    "    return samples[:n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from assignment_files.tokenizer import Tokenizer\n",
    "\n",
    "def analyze_compression(tokenizer: Tokenizer, documents: list[str], name: str):\n",
    "    \"\"\"\n",
    "    Encodes documents and calculates compression ratio and throughput.\n",
    "    \"\"\"\n",
    "    total_bytes = sum(len(d.encode('utf-8')) for d in documents)\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for token in tokenizer.encode_iterable(documents):\n",
    "        total_tokens += 1\n",
    "        \n",
    "    duration = time.perf_counter() - start_time\n",
    "    \n",
    "    ratio = total_bytes / total_tokens if total_tokens > 0 else 0\n",
    "    throughput = total_bytes / duration if duration > 0 else 0\n",
    "    \n",
    "    print(f\"[{name}]\")\n",
    "    print(f\"  Compression Ratio: {ratio:.2f} bytes/token\")\n",
    "    print(f\"  Throughput:        {throughput / 1e6:.2f} MB/s\")\n",
    "    return ratio, throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from assignment_files.bpe_tokenizer import find_chunk_boundaries\n",
    "from assignment_files.tokenizer import Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def _encode_worker(task_tuple, tokenizer):\n",
    "    file_path, start, end = task_tuple\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        f.seek(start)\n",
    "        chunk_bytes = f.read(end - start)\n",
    "    \n",
    "    # Decode bytes to string for the tokenizer\n",
    "    text = chunk_bytes.decode('utf-8', errors='replace')\n",
    "    \n",
    "    # Encode using tokenizer\n",
    "    tokens = tokenizer.encode(text)\n",
    "    \n",
    "    # Return as uint16\n",
    "    return np.array(tokens, dtype=np.uint16)\n",
    "\n",
    "def encode_dataset(tokenizer, input_filepath, output_filepath, procs=None):\n",
    "    procs = procs or os.cpu_count()\n",
    "    special_tokens_bytes = [t.encode('utf-8') for t in tokenizer.special_tokens]\n",
    "\n",
    "    # find chunk boundries\n",
    "    print(f\"Calculating boundaries for {input_filepath}...\")\n",
    "    with open(input_filepath, 'rb') as f:\n",
    "        boundaries = find_chunk_boundaries(f, procs * 4, special_tokens_bytes)\n",
    "\n",
    "    # Task parameters (file_path, start_byte, end_byte)\n",
    "    tasks = [(input_filepath, boundaries[i], boundaries[i+1]) \n",
    "             for i in range(len(boundaries) - 1)]\n",
    "\n",
    "    # Process chunks in parallel and stream to a temporary binary file\n",
    "    temp_bin = output_filepath + \".tmp\"\n",
    "    total_tokens = 0\n",
    "    \n",
    "    # Partial gives tokenizer instance to every worker\n",
    "    worker_with_tokenizer = partial(_encode_worker, tokenizer=tokenizer)\n",
    "\n",
    "    print(f\"Encoding {len(tasks)} chunks using {procs} processes...\")\n",
    "    with open(temp_bin, 'wb') as f_out:\n",
    "        with multiprocessing.Pool(processes=procs) as pool:\n",
    "            # imap allows us to process results as they finish\n",
    "            for token_arr in tqdm(pool.imap(worker_with_tokenizer, tasks), total=len(tasks)):\n",
    "                f_out.write(token_arr.tobytes())\n",
    "                total_tokens += len(token_arr)\n",
    "\n",
    "    # Raw binary -> NumPy array -> .npy file\n",
    "    print(f\"Finalizing {total_tokens} tokens...\")\n",
    "    # We use mmap_mode if the file is truly massive, otherwise fromfile is fine\n",
    "    final_data = np.fromfile(temp_bin, dtype=np.uint16)\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n",
    "    np.save(output_filepath, final_data)\n",
    "    \n",
    "    # Cleanup\n",
    "    if os.path.exists(temp_bin):\n",
    "        os.remove(temp_bin)\n",
    "        \n",
    "    print(f\"Completed! Saved to {output_filepath}\")\n",
    "    print(f\"Final size: {final_data.nbytes / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing tokenizers...\n",
      "Sampling 10 documents from ../data/TinyStoriesV2-GPT4-train.txt...\n",
      "Sampling 10 documents from ../data/owt_train.txt...\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing tokenizers...\")\n",
    "\n",
    "ts_tokenizer = Tokenizer.from_files(\n",
    "    vocab_filepath=\"assignment_files/TinyStoriesVocab.json\",\n",
    "    merges_filepath=\"assignment_files/TinyStoriesMerges.json\",\n",
    "    special_tokens=[\"<|endoftext|>\"]\n",
    ")\n",
    "\n",
    "owt_tokenizer = Tokenizer.from_files(\n",
    "    vocab_filepath=\"assignment_files/OWTVocab.json\",\n",
    "    merges_filepath=\"assignment_files/OWTMerges.json\",\n",
    "    special_tokens=[\"<|endoftext|>\"]\n",
    ")\n",
    "\n",
    "# Sample 10 documents\n",
    "ts_samples = get_sample_documents(\"../data/TinyStoriesV2-GPT4-train.txt\", n_samples=10)\n",
    "owt_samples = get_sample_documents(\"../data/owt_train.txt\", n_samples=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TinyStories Tokenizer on TinyStories samples]\n",
      "  Compression Ratio: 4.15 bytes/token\n",
      "  Throughput:        0.96 MB/s\n",
      "[OWT Tokenizer on OWT Samples]\n",
      "  Compression Ratio: 4.69 bytes/token\n",
      "  Throughput:        0.90 MB/s\n",
      "[TinyStories Tokenizer on OWT Samples]\n",
      "  Compression Ratio: 3.19 bytes/token\n",
      "  Throughput:        0.31 MB/s\n",
      "\n",
      "[The Pile Estimation]\n",
      "  Time to tokenize 825GB: 253.95 hours\n"
     ]
    }
   ],
   "source": [
    "# (a) Sample 10 documents from TinyStories and OpenWebText. Using your previously-trained TinyStories and OpenWebText tokenizers encode these \n",
    "# sampled documents into integer IDs. What is each tokenizer’s compression ratio (bytes/token)?\n",
    "analyze_compression(ts_tokenizer, ts_samples, \"TinyStories Tokenizer on TinyStories samples\")\n",
    "_, owt_throughput = analyze_compression(owt_tokenizer, owt_samples, \"OWT Tokenizer on OWT Samples\")\n",
    "\n",
    "# (b)  What happens if you tokenize your OpenWebText sample with the TinyStories tokenizer? Compare the compression ratio and/or qualitatively describe what happens.\n",
    "analyze_compression(ts_tokenizer, owt_samples, \"TinyStories Tokenizer on OWT Samples\")\n",
    "\n",
    "# (c) Estimate the throughput of your tokenizer (e.g., in bytes/second). How long would it take to tokenize the Pile dataset (825GB of text)\n",
    "pile_size_gb = 825\n",
    "seconds_needed = (pile_size_gb * 1e9) / owt_throughput\n",
    "print(f\"\\n[The Pile Estimation]\")\n",
    "print(f\"  Time to tokenize 825GB: {seconds_needed / 3600:.2f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating boundaries for ../data/TinyStoriesV2-GPT4-train.txt...\n",
      "Encoding 159 chunks using 40 processes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159/159 [11:48<00:00,  4.45s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalizing 537848181 tokens...\n",
      "Completed! Saved to assignment_files/tinystories_train.npy\n",
      "Final size: 1025.86 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# (d) Using your TinyStories and OpenWebText tokenizers, encode the respective training and development datasets into a sequence of integer token ID\n",
    "encode_dataset(ts_tokenizer, \"../data/TinyStoriesV2-GPT4-train.txt\", \"../data/tinystories_train.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating boundaries for ../data/owt_train.txt...\n",
      "Encoding 159 chunks using 40 processes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159/159 [1:05:44<00:00, 24.81s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalizing 1810995702 tokens...\n",
      "Completed! Saved to ../data/owt_train.npy\n",
      "Final size: 3454.20 MB\n"
     ]
    }
   ],
   "source": [
    "encode_dataset(owt_tokenizer, \"../data/owt_train.txt\", \"../data/owt_train.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNHqTHZ+dXPCdjCPYlfqpV1",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
