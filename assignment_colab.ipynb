{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGV11RtlneMZ"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Anson2Leung/ece405-assignment1-basics.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Install uv\n",
        "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
        "\n",
        "# Add uv to the system path so you can use it directly\n",
        "import os\n",
        "os.environ['PATH'] = f\"{os.path.expanduser('~/.cargo/bin')}:{os.environ['PATH']}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd ece405-assignment1-basics\n",
        "\n",
        "!uv pip install -r requirements.txt --system\n",
        "!pip install \"numpy<2.0\" jaxtyping typeguard langsmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pytest tests/test_train_bpe.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data files\n",
        "%cd ..\n",
        "%mkdir -p data\n",
        "%cd data\n",
        "\n",
        "!wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt\n",
        "!wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-valid.txt\n",
        "\n",
        "!wget https://huggingface.co/datasets/stanford-cs336/owt-sample/resolve/main/owt_train.txt.gz\n",
        "!gunzip owt_train.txt.gz\n",
        "!wget https://huggingface.co/datasets/stanford-cs336/owt-sample/resolve/main/owt_valid.txt.gz\n",
        "!gunzip owt_valid.txt.gz\n",
        "\n",
        "%cd ..\n",
        "%cd ece405-assignment1-basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problem (train_bpe_tinystories): BPE Training on TinyStories (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import json\n",
        "\n",
        "# 1. Ensure the script is in the path\n",
        "%cd /content/ece405-assignment1-basics\n",
        "sys.path.append('/content/ece405-assignment1-basics/assignment_files')\n",
        "from bpe_tokenizer import train_bpe, save_vocab, save_merges\n",
        "\n",
        "# 2. Configuration\n",
        "INPUT_PATH = \"/content/data/TinyStoriesV2-GPT4-train.txt\"\n",
        "VOCAB_SIZE = 10000\n",
        "SPECIAL_TOKENS = [\"<|endoftext|>\"]\n",
        "\n",
        "# 3. Execution\n",
        "# We call the function directly. Your code already handles the timing and memory printing.\n",
        "vocab, merges = train_bpe(INPUT_PATH, VOCAB_SIZE, SPECIAL_TOKENS)\n",
        "\n",
        "# 4. Serialization\n",
        "save_vocab(vocab, \"vocab.json\")\n",
        "save_merges(merges, \"merges.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analysis\n",
        "def get_decoded_string(token_bytes):\n",
        "    return token_bytes.decode('utf-8', errors='ignore')\n",
        "\n",
        "# Find the longest token in the vocabulary\n",
        "longest_token_id = max(vocab, key=lambda k: len(get_decoded_string(vocab[k])))\n",
        "longest_token_bytes = vocab[longest_token_id]\n",
        "longest_text = get_decoded_string(longest_token_bytes)\n",
        "\n",
        "print(f\"--- Analysis Results ---\")\n",
        "print(f\"Longest Token ID: {longest_token_id}\")\n",
        "print(f\"Longest Token Text: '{longest_text}'\")\n",
        "print(f\"Character Length: {len(longest_text)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOIKMwK1q94rO3aoVq6DEA6",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
